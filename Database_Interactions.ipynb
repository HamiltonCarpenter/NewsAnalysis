{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper Functions Included\n"
     ]
    }
   ],
   "source": [
    "%run 'Misc_Helper_Funcs.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database_Interactions Included\n"
     ]
    }
   ],
   "source": [
    "print(\"Database_Interactions Included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_scrape_table(mycursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatabaseManager:\n",
    "\n",
    "    def __init__(self):\n",
    "        #def default_newsdatabase_connect():\n",
    "        self.mydb = mysql.connector.connect(\n",
    "          host=\"localhost\",\n",
    "          user=\"pythonuser\",\n",
    "          passwd=\"password\",\n",
    "          database=\"newsdatabase\"\n",
    "        ) \n",
    "\n",
    "        self.mycursor = self.mydb.cursor(buffered=True)\n",
    "#    return self.mycursor\n",
    "#self.mycursor = default_newsdatabase_connect()\n",
    "\n",
    "    def get_date_for_url_db(self, url):\n",
    "        sql = \"SELECT date FROM urls WHERE url= %s\"\n",
    "        val = (url,)\n",
    "\n",
    "        self.mycursor.execute(sql,val)\n",
    "        row = self.mycursor.fetchall()\n",
    "        assert(1 == len(row))\n",
    "        db_date = row[0][0]\n",
    "        obj_date = convert_m_d_y_to_date_object(db_date)\n",
    "        return obj_date\n",
    "\n",
    "    def get_outlet_for_url_db(self, url):\n",
    "        sql = \"SELECT outlet FROM urls WHERE url= %s\"\n",
    "        val = (url,)\n",
    "\n",
    "        self.mycursor.execute(sql,val)\n",
    "        row = self.mycursor.fetchall()\n",
    "        assert(1 == len(row))\n",
    "        outlet = row[0][0]\n",
    "        return outlet\n",
    "\n",
    "    def set_path_for_file_db(self, url, file_loc):\n",
    "        self.mycursor = self.mydb.cursor()\n",
    "        sql = \"UPDATE Rawtext SET File_Location = %s WHERE URL = %s\"\n",
    "        val = (file_loc, url)\n",
    "        self.mycursor.execute(sql,val)\n",
    "        self.mydb.commit()\n",
    "\n",
    "    def mark_url_as_scraped_db(self, url):\n",
    "        self.mycursor = self.mydb.cursor()\n",
    "        sql = \"UPDATE Rawtext SET Scraped = 1 WHERE URL = %s\"\n",
    "        val = (url,)\n",
    "        self.mycursor.execute(sql,val)\n",
    "        self.mydb.commit()\n",
    "\n",
    "    def get_unscraped_urls_from_db(self):\n",
    "        sql = \"SELECT * FROM Rawtext WHERE Scraped = 0\"\n",
    "\n",
    "        self.mycursor.execute(sql)\n",
    "        unscrapped = self.mycursor.fetchall()\n",
    "        return unscrapped\n",
    "\n",
    "\n",
    "    def check_present(self,url):\n",
    "        t = (url,)\n",
    "        #print(self.mycursor.execute('SELECT * FROM urls WHERE id=2'))\n",
    "        self.mycursor.execute('SELECT * FROM urls WHERE url=%s', t)\n",
    "        match = self.mycursor.fetchall()\n",
    "        return len(match) > 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_unscraped_article(self, url, file_location):\n",
    "        #self.mycursor = self.mydb.cursor()\n",
    "\n",
    "        sql = \"UPDATE Rawtext SET File_Location = %s WHERE URL = %s\"\n",
    "        val = (file_location, URL)\n",
    "\n",
    "        self.mycursor.execute(sql, val)\n",
    "\n",
    "        self.mydb.commit()\n",
    "\n",
    "        print(self.mycursor.rowcount, \"record(s) affected\") \n",
    "\n",
    "    def check_article_rawtext_present(self, url):\n",
    "        t = (url,)\n",
    "        #print(self.mycursor.execute('SELECT * FROM urls WHERE id=2'))\n",
    "        self.mycursor.execute('SELECT * FROM check WHERE url=%s', t)\n",
    "        match = self.mycursor.fetchall()\n",
    "        return len(match) > 0\n",
    "\n",
    "    def write_raw_text_to_db(self, url, rawtext):\n",
    "        if not check_article_rawtext_present(construct_full_url(url)):\n",
    "            #print(construct_full_url('/2019/08/19/us/politics/elizabeth-warren-native-american.html', 'NewYorkTimes'))\n",
    "            sql = \"INSERT INTO raw_articles (url, raw_article) VALUES (%s, %s)\"\n",
    "            val = (construct_full_url(link, outlet),date,outlet) #(\"https://www.nytimes.com\"+link, \"08/20/2019\",NewYorkTimes)\n",
    "            self.mycursor.execute(sql, val)\n",
    "\n",
    "            self.mydb.commit()\n",
    "\n",
    "            print(self.mycursor.rowcount, \"record inserted.\")\n",
    "\n",
    "    def update_scrape_table(self):\n",
    "        #connect to db\n",
    "\n",
    "        sql = \"SELECT * FROM urls WHERE url NOT IN (SELECT URL FROM Rawtext)\"\n",
    "\n",
    "        self.mycursor.execute(sql)\n",
    "        unscrapped = self.mycursor.fetchall()\n",
    "\n",
    "        insert_sql = \"INSERT INTO Rawtext (Scraped, URL, File_Location) VALUES (%s, %s, %s)\"\n",
    "        for x in unscrapped:\n",
    "\n",
    "            URL=x[0]\n",
    "            Scraped = 0\n",
    "            File_Location= None\n",
    "\n",
    "            insertval = (Scraped,URL,File_Location) #(\"https://www.nytimes.com\"+link, \"08/20/2019\",NewYorkTimes)\n",
    "            self.mycursor.execute(insert_sql, insertval)\n",
    "\n",
    "            self.mydb.commit()\n",
    "            print(URL+\" Transered\")\n",
    "\n",
    "    def update_parsed_table(self):\n",
    "        #connect to db\n",
    "\n",
    "        sql = \"SELECT URL FROM Rawtext WHERE URL NOT IN (SELECT URL FROM Parsedtext)\"\n",
    "\n",
    "        self.mycursor.execute(sql)\n",
    "        unscrapped = self.mycursor.fetchall()\n",
    "\n",
    "        insert_sql = \"INSERT INTO Parsedtext (Parsed, URL, File_Location) VALUES (%s, %s, %s)\"\n",
    "        for x in unscrapped:\n",
    "\n",
    "            URL=x[0]\n",
    "            Parsed = 0\n",
    "            File_Location= None\n",
    "\n",
    "            insertval = (Parsed,URL,File_Location) #(\"https://www.nytimes.com\"+link, \"08/20/2019\",NewYorkTimes)\n",
    "            self.mycursor.execute(insert_sql, insertval)\n",
    "\n",
    "            self.mydb.commit()\n",
    "            print(URL+\" Transered\")\n",
    "            \n",
    "    def get_uncleaned_urls_from_db(self):\n",
    "        sql = \"SELECT URL FROM Parsedtext WHERE Parsed = %s\"\n",
    "        val = (0,)\n",
    "\n",
    "        self.mycursor.execute(sql,val)\n",
    "        unscrapped = self.mycursor.fetchall()\n",
    "        return unscrapped\n",
    "        \n",
    "    def set_cleaned_file_location(self,url,file_location):\n",
    "        sql = \"UPDATE Parsedtext SET File_Location = %s, Parsed = %s WHERE URL = %s\"\n",
    "        val = (file_location, 1, url)\n",
    "\n",
    "        self.mycursor.execute(sql, val)\n",
    "\n",
    "        self.mydb.commit()\n",
    "\n",
    "        print(self.mycursor.rowcount, \"record(s) affected\") \n",
    "        \n",
    "        \n",
    "        #updates urls that are duplicates as such\n",
    "        #refactor into a separate deduplicate function\n",
    "        #probably want to convert form to importing all available urls then annotate \n",
    "        #convert to return list of urls not list of single tuples, remove [0] in duplicate \n",
    "    def get_unannotated_urls(self):\n",
    "        #connect to db\n",
    "\n",
    "        sql = \"SELECT URL FROM urls WHERE URL NOT IN (SELECT url FROM StorySummaries)\"\n",
    "\n",
    "        self.mycursor.execute(sql)\n",
    "        unscrapped = self.mycursor.fetchall()\n",
    "        \n",
    "        urls = []\n",
    "        \n",
    "        #change how this works, dont want to load everything into list, bad form\n",
    "        for x in unscrapped: \n",
    "            #x = article_link[0]#rename x\n",
    "            same_url = self.check_if_duplicate_url_already_in_story_summaries(x)\n",
    "            if None == same_url:#no duplicate present\n",
    "                urls.append(x)\n",
    "            else:\n",
    "                url = x\n",
    "                descriptors = []\n",
    "                same_stories = []\n",
    "                exact_duplicates = [same_url]\n",
    "                summary = \"Duplicate\"\n",
    "\n",
    "                print(\"current url: \",x)\n",
    "                print(\"duplicate url: \", same_url)\n",
    "                self.add_summary_to_db(url, descriptors, same_stories, exact_duplicates, summary)\n",
    "        return urls\n",
    "    \n",
    "    def write_urls_to_db(self, url_list, outlet, category, date):\n",
    "        for link in url_list:\n",
    "            if not self.check_present(construct_full_url(link, outlet)):\n",
    "                #print(construct_full_url('/2019/08/19/us/politics/elizabeth-warren-native-american.html', 'NewYorkTimes'))\n",
    "                sql = \"INSERT INTO urls (url, date, outlet) VALUES (%s, %s, %s)\"\n",
    "                val = (construct_full_url(link, outlet),date,outlet) #(\"https://www.nytimes.com\"+link, \"08/20/2019\",NewYorkTimes)\n",
    "                self.mycursor.execute(sql, val)\n",
    "\n",
    "                self.mydb.commit()\n",
    "\n",
    "                print(self.mycursor.rowcount, \"record inserted.\")\n",
    " \n",
    "    def add_summary_to_db(self, url, descriptors, same_stories, exact_duplicates, summary):\n",
    "        sql = \"INSERT INTO StorySummaries (url, descriptors, same_stories, exact_duplicates, summary) VALUES (%s, %s, %s, %s, %s)\"\n",
    "        val = (url,json.dumps({'descriptors':descriptors}),json.dumps({'same_stories':same_stories}),json.dumps({'exact_duplicates':exact_duplicates}),summary) \n",
    "        #(\"https://www.nytimes.com\"+link, \"08/20/2019\",NewYorkTimes)\n",
    "        self.mycursor.execute(sql, val)\n",
    "\n",
    "        self.mydb.commit()\n",
    "\n",
    "        print(self.mycursor.rowcount, \"record inserted.\")\n",
    "                \n",
    "            \n",
    "            #should set up system to do this based on parsed text\n",
    "    def check_if_duplicate_url_already_in_story_summaries(self,url):\n",
    "        if \"washingtonpost.com\" in url:\n",
    "            #not yet tagged\n",
    "            inner_text = url.split(\"washingtonpost.com\")[1]\n",
    "            inner_text = inner_text.split(\".html\")[0]\n",
    "            inner_text = '%'+inner_text+'%'\n",
    "            sql = \"SELECT url FROM StorySummaries WHERE url LIKE %s\"\n",
    "            val = (inner_text,)\n",
    "            \n",
    "            self.mycursor.execute(sql,val)\n",
    "            duplicates = self.mycursor.fetchall()\n",
    "            \n",
    "            #insert general purpose high threshold bag of words checker \n",
    "            assert len(duplicates)<2 # if i see more than 1 something has gone wrong, my method of checking for duplication involves checking for known issues \n",
    "            if 1 == len(duplicates):\n",
    "                return duplicates[0][0]\n",
    "            else: # no matches\n",
    "                return None\n",
    "        else: \n",
    "            return None\n",
    "        \n",
    "        \n",
    "    def add_url_as_duplicate(self, url_to_add, aready_present_url):\n",
    "        pass \n",
    "        add_summary_to_db(self, url, descriptors, same_stories, exact_duplicates, summary)\n",
    "        \n",
    "        \n",
    "    #return the head \n",
    "        \n",
    "    def identify_duplicate_stories_summaries_db(self):\n",
    "        \n",
    "        \n",
    "        url = current_article\n",
    "        descriptors = []#['Wilbur Ross','Huawei', 'trade war',\"sanctions\"]\n",
    "        same_stories = []#['https://www.washingtonpost.com/politics/trump-again-undercuts-his-administrations-messy-tariffs-rhetoric/2019/08/19/6ca7752a-0a75-46cb-8b5f-096496acc028_story.html']\n",
    "        exact_duplicates = ['https://www.washingtonpost.com/politics/after-two-more-mass-shootings-trump-again-appears-to-back-away-from-gun-background-checks/2019/08/19/ddbf75e4-c2af-11e9-b72f-b31dfaa77212_story.html']\n",
    "        summary = \"Duplicate\" #\"Huawei gets another 90 day extention on sanctions\"\n",
    "\n",
    "        db.add_summary_to_db(url, descriptors, same_stories, exact_duplicates, summary)\n",
    "        'https://www.washingtonpost.com/politics/omar-tlaib-blast-israel-for-blocking-their-visit/2019/08/19/6c0ffda6-c2b9-11e9-850e-c0eef81a5224_story.html'\n",
    "        'http://washingtonpost.com/politics/omar-tlaib-blast-israel-for-blocking-their-visit/2019/08/19/6c0ffda6-c2b9-11e9-850e-c0eef81a5224_story.html?tid=pm_politics_pop'\n",
    "    \n",
    "        if \"washingtonpost.com\" in url:\n",
    "            #not yet tagged\n",
    "            inner_text = url.split(\"washingtonpost.com\")[1]\n",
    "            inner_text = url.split(\".html\")[0]\n",
    "        else:\n",
    "            11+1\n",
    "            \n",
    "    #url varchar(1000), descriptors json, same_stories json, exact_duplicates json, summary varchar(10000), id MEDIUMINT NOT NULL AUTO_INCREMENT,\n",
    "                \n",
    "    #change to open new connection\n",
    "    def open_connection_db(self):\n",
    "        self.mydb = mysql.connector.connect(\n",
    "          host=\"localhost\",\n",
    "          user=\"pythonuser\",\n",
    "          passwd=\"password\",\n",
    "          database=\"newsdatabase\"\n",
    "        ) \n",
    "        self.mycursor = self.mydb.cursor(buffered=True) \n",
    "        return self.mycursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_data = json.dumps({'meme':'memeval'})\n",
    "\n",
    "#loaded_json = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 record inserted.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# a Python object (dict):\n",
    "#x = {\n",
    "#  \"name\": \"John\",\n",
    "#  \"age\": 30,\n",
    "#  \"city\": \"New York\"\n",
    "#}\n",
    "\n",
    "# convert into JSON:\n",
    "#y = json.dumps(x)\n",
    "\n",
    "# the result is a JSON string:\n",
    "#print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mysql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6d793c22e2ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_if_duplicate_url_already_in_story_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://washingtonpost.com/politics/omar-tlaib-blast-israel-for-blocking-their-visit/2019/08/19/6c0ffda6-c2b9-11e9-850e-c0eef81a5224_story.html?tid=pm_politics_pop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c66040bab904>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#def default_newsdatabase_connect():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         self.mydb = mysql.connector.connect(\n\u001b[0m\u001b[1;32m      7\u001b[0m           \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pythonuser\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mysql' is not defined"
     ]
    }
   ],
   "source": [
    "#db = DatabaseManager()\n",
    "\n",
    "#db.check_if_duplicate_url_already_in_story_summaries(\"http://washingtonpost.com/politics/omar-tlaib-blast-israel-for-blocking-their-visit/2019/08/19/6c0ffda6-c2b9-11e9-850e-c0eef81a5224_story.html?tid=pm_politics_pop\")\n",
    "\n",
    "\n",
    "\n",
    "#db.add_summary_to_db('url', 'outlet', 'same_stories', 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TABLE StorySummaries(url varchar(1000), descriptors json, same_stories json, exact_duplicates json, summary varchar(10000), id MEDIUMINT NOT NULL AUTO_INCREMENT, PRIMARY KEY (id) );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
