{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc_helper_funcs_and_constants import *\n",
    "from database_interactions import *  \n",
    "from local_file_interactions import * \n",
    "\n",
    "def scrape_section_for_urls(outlet,section_identifier,url_storage):\n",
    "    http = urllib3.PoolManager()\n",
    "\n",
    "    response = http.request('GET', url_storage[outlet][section_identifier])\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    #print(soup.prettify())\n",
    "    s = soup.find_all('a', href=True)\n",
    "\n",
    "    valid_links = []\n",
    "    for link in s:\n",
    "        #keep non text pages out \n",
    "        if link['href'].find('politics') != -1 and link['href'].find('.html') != -1 and link['href'].find('/interactive/') == -1 and link['href'].find('/video/') == -1 and link['href'].find('index.html') == -1:\n",
    "            if link['href'] not in valid_links:\n",
    "                valid_links.append(link['href'])\n",
    "    return valid_links\n",
    "\n",
    "def scrape_outlets_for_urls(url_storage):\n",
    "    for outlet in url_storage:\n",
    "        for sec_url_pair in url_storage[outlet]:\n",
    "            pass\n",
    "#def convert_urls():\n",
    "#    pass\n",
    "\n",
    "\n",
    "\n",
    "#refactor so im not directely indexing response tuples\n",
    "#update the section here\n",
    "def scrape_unscraped_articles():\n",
    "    lf = LocalFileHandler()\n",
    "    db = DatabaseManager()\n",
    "\n",
    "    unscrapped_articles = db.get_unscraped_urls_from_db()\n",
    "    doc_status = \"Raw\"\n",
    "    category = \"Politics\"\n",
    "    #REPLACE HARDCODES\n",
    "    for article in unscrapped_articles:\n",
    "        url = article[1]\n",
    "        \n",
    "        date = db.get_date_for_url_db(url)\n",
    "        outlet = db.get_outlet_for_url_db(url)\n",
    "        organizational_path = lf.generate_organizational_path(doc_status,outlet,category,date)\n",
    "        file_path=lf.find_absolute_file_path(organizational_path) #directory structure above the file without the name\n",
    "        \n",
    "        #REEPLACE POLITICS HARDCODE\n",
    "        \n",
    "        raw_text = scrape_article_for_raw_text(outlet,category,url)        \n",
    "        filename = lf.convert_url_to_filename(url)\n",
    "        \n",
    "        lf.update_path_structure(organizational_path) #create file hirarchy, takes only organization as input\n",
    "        lf.write_text_to_location(raw_text,filename,file_path)\n",
    "        \n",
    "        file_loc=lf.find_absolute_document_location(organizational_path,filename) #directory structure with file name\n",
    "        db.set_path_for_file_db(url, file_loc)\n",
    "        \n",
    "        db.mark_url_as_scraped_db(url)\n",
    "        print(\"test complete for: \")\n",
    "        print(url)\n",
    "        #break #for testing 1 update\n",
    "        time.sleep(60+random.randrange(0,60))\n",
    "\n",
    "        #move raw below the date\n",
    "        \n",
    "def scrape_article_for_raw_text(outlet,section,url):\n",
    "    http = urllib3.PoolManager()\n",
    "\n",
    "    response = http.request('GET', url)\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    #print(soup.prettify())\n",
    "    return str(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
